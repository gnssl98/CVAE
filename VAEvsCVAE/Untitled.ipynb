{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77c1561-724b-4328-bac6-8b9b582c85c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc34c2c-05f7-4f4b-99ca-7544f71866fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Lambda, Input, Dense\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy, kl_divergence\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PowerTransformer\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfcb336-ffc6-411f-be1e-750c5879e8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:/dataset/cleaned_improved_cicids2017.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b021409-3292-44d1-b234-2a02a5edcfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "tmp = df.drop(labels = 'Label',axis=1)\n",
    "labels = df['Label']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(tmp)\n",
    "\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=tmp.columns)  # 스케일된 데이터를 DataFrame으로 변환\n",
    "df_merged = pd.concat([X_scaled_df, labels.reset_index(drop=True)], axis=1)  # Index 정렬 후 병합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776d0af4-3c2e-4705-82e5-06bbbfacaf5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test = train_test_split(df_merged, test_size=0.1, random_state=42)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360729f4-4bde-40c6-ab17-fbacb17a8eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_normal = X_train[X_train['Label'] == 0]\n",
    "X_train_normal.shape\n",
    "\n",
    "X_test_normal = X_test[X_test['Label'] == 0]\n",
    "X_test_normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac41abb-492c-4feb-89c8-be9ca7fce593",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_not_normal = X_train[X_train['Label'] !=0]\n",
    "\n",
    "X_test = pd.concat([X_test, X_train_not_normal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b23028-10cc-48d3-a85a-7a6697bde56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = X_test['Label']\n",
    "X_test = X_test.drop(labels='Label',axis=1)\n",
    "y_train_normal = X_train_normal['Label']\n",
    "X_train_normal = X_train_normal.drop(labels='Label', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921d4087-9807-4f6c-8034-f563457cfa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, RepeatVector, Conv1D, Conv1DTranspose\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "from keras_self_attention import SeqWeightedAttention, SeqSelfAttention\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7262c97-e865-4d5d-8733-564d139d85d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses, Model, Input\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "latent_dim = 10\n",
    "inter_dim = 20\n",
    "\n",
    "# Sampling function for reparameterization trick\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    batch_size = tf.shape(z_mean)[0]\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=1.)\n",
    "    return z_mean + z_log_sigma * epsilon\n",
    "\n",
    "def vae_loss(x, x_decoded_mean, z_mean, z_log_sigma):\n",
    "    reconstruction_loss = K.sum(K.square(x - x_decoded_mean), axis=1)\n",
    "    kl_loss = -0.5 * K.sum(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "    kl_loss_weighted = kl_loss * 0.001  # KL 다이버전스 손실에 가중치 부여\n",
    "    total_loss = K.mean(reconstruction_loss + kl_loss_weighted)\n",
    "    return total_loss\n",
    "\n",
    "def cvae(X, labels):\n",
    "    features = X.shape[1]  # 2D 데이터이므로 timesteps는 필요 없고, features만 사용\n",
    "    input_x = Input(shape=(features,), name='InputFeatures')  # 2D 입력 (샘플, 특징)\n",
    "\n",
    "    input_label = Input(shape=(1,), name='InputLabel')  # Assumes binary or multi-class label\n",
    "\n",
    "    # Embed the label into the same shape as the input\n",
    "    embedded_label = layers.Embedding(input_dim=2, output_dim=features)(input_label)  # Adjust output_dim to match features\n",
    "    embedded_label = layers.Flatten()(embedded_label)\n",
    "\n",
    "    # Concatenate input_x and embedded_label\n",
    "    concatenated_input = layers.Concatenate()([input_x, embedded_label])\n",
    "\n",
    "    # Reshape for Conv1D layer (to 3D)\n",
    "    reshaped_input = layers.Reshape((features + embedded_label.shape[-1], 1))(concatenated_input)\n",
    "\n",
    "    # Encoder with CNN layers\n",
    "    h = layers.Conv1D(filters=64, kernel_size=3, activation=\"relu\", padding='same')(reshaped_input)\n",
    "    h = layers.Conv1D(filters=32, kernel_size=3, activation=\"relu\", padding='same')(h)\n",
    "    h = layers.Conv1D(filters=16, kernel_size=3, activation=\"relu\", padding='same')(h)\n",
    "\n",
    "    # Flatten for dense layers (MLP)\n",
    "    h = layers.Flatten()(h)\n",
    "    h = layers.Dense(inter_dim, activation='relu')(h)\n",
    "    h = layers.Dense(inter_dim, activation='relu')(h)\n",
    "\n",
    "    # z layer\n",
    "    z_mean = layers.Dense(latent_dim)(h)\n",
    "    z_log_sigma = layers.Dense(latent_dim)(h)\n",
    "    z = layers.Lambda(sampling)([z_mean, z_log_sigma])\n",
    "\n",
    "    # Decoder\n",
    "    # Add the label to the latent space z\n",
    "    z_with_label = layers.Concatenate()([z, embedded_label])\n",
    "\n",
    "    # Expand for Conv1DTranspose layer\n",
    "    decoder1 = layers.Dense((features + embedded_label.shape[-1]) * inter_dim)(z_with_label)\n",
    "    decoder1 = layers.Reshape((features + embedded_label.shape[-1], inter_dim))(decoder1)\n",
    "\n",
    "    # Decoder with CNN layers\n",
    "    decoder1 = layers.Conv1DTranspose(filters=16, kernel_size=3, activation=\"relu\", padding='same')(decoder1)\n",
    "    decoder1 = layers.Conv1DTranspose(filters=32, kernel_size=3, activation=\"relu\", padding='same')(decoder1)\n",
    "    decoder1 = layers.Conv1DTranspose(filters=64, kernel_size=3, activation=\"relu\", padding='same')(decoder1)\n",
    "\n",
    "    # Flatten and final dense layer to reconstruct original features\n",
    "    decoder1 = layers.Flatten()(decoder1)\n",
    "    decoder1 = layers.Dense(features)(decoder1)\n",
    "    \n",
    "    model = Model([input_x, input_label], decoder1)\n",
    "    model.add_loss(vae_loss(input_x, decoder1, z_mean, z_log_sigma))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create CVAE model\n",
    "model = cvae(X_train_normal, y_train_normal)\n",
    "model.summary()\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), metrics=['accuracy'])\n",
    "# Create an EarlyStopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Compile and train the model with early stopping\n",
    "history = model.fit([X_train_normal, y_train_normal], X_train_normal,\n",
    "                    shuffle=True,\n",
    "                    epochs=50, \n",
    "                    validation_split=0.1,  \n",
    "                    batch_size=32,\n",
    "                    callbacks=[early_stopping]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4934681e-22b6-47ff-9ec4-0b5149ad8608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, losses, Model, Input\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "latent_dim = 10  # Latent 공간 차원\n",
    "inter_dim = 20   # 중간 층 차원\n",
    "\n",
    "# Sampling function for reparameterization trick\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    batch_size = tf.shape(z_mean)[0]\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=1.)\n",
    "    return z_mean + z_log_sigma * epsilon\n",
    "\n",
    "# VAE 손실 함수 정의\n",
    "def vae_loss(x, x_decoded_mean, z_mean, z_log_sigma):\n",
    "    reconstruction_loss = K.sum(K.square(x - x_decoded_mean), axis=1)  # MSE 기반 복원 손실\n",
    "    kl_loss = -0.5 * K.sum(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)  # KL 다이버전스\n",
    "    kl_loss_weighted = kl_loss * 0.0001  # KL 손실 가중치 적용\n",
    "    total_loss = K.mean(reconstruction_loss + kl_loss_weighted)\n",
    "    return total_loss\n",
    "\n",
    "# VAE 모델 생성\n",
    "def vae(X_train):\n",
    "    features = X_train.shape[1]  # 입력 차원\n",
    "    input_x = Input(shape=(features,), name='InputFeatures')  # 2D 입력 (샘플, 특징)\n",
    "\n",
    "    # Encoder\n",
    "    h = layers.Dense(128, activation='relu')(input_x)\n",
    "    h = layers.Dense(64, activation='relu')(h)\n",
    "    h = layers.Dense(inter_dim, activation='relu')(h)\n",
    "\n",
    "    # Latent space\n",
    "    z_mean = layers.Dense(latent_dim)(h)\n",
    "    z_log_sigma = layers.Dense(latent_dim)(h)\n",
    "    z = layers.Lambda(sampling)([z_mean, z_log_sigma])\n",
    "\n",
    "    # Decoder\n",
    "    decoder = layers.Dense(inter_dim, activation='relu')(z)\n",
    "    decoder = layers.Dense(64, activation='relu')(decoder)\n",
    "    decoder = layers.Dense(128, activation='relu')(decoder)\n",
    "    decoder = layers.Dense(features, activation='linear')(decoder)  # 원래 feature 차원으로 복원\n",
    "\n",
    "    model = Model(input_x, decoder)\n",
    "    model.add_loss(vae_loss(input_x, decoder, z_mean, z_log_sigma))  # VAE Loss 추가\n",
    "\n",
    "    return model\n",
    "\n",
    "# VAE 모델 생성\n",
    "model = vae(X_train_normal)\n",
    "model.summary()\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005))\n",
    "\n",
    "# EarlyStopping 콜백\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(X_train_normal, X_train_normal,\n",
    "                    shuffle=True,\n",
    "                    epochs=100,\n",
    "                    validation_split=0.1,\n",
    "                    batch_size=32,\n",
    "                    callbacks=[early_stopping]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e45b242-09ca-431f-942c-4eaf92e3e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "\n",
    "# VAE 및 CVAE에서 추출한 latent space 벡터 (예제 데이터)\n",
    "z_vae = np.random.randn(5000, 10)  # (5000개 샘플, 10차원 잠재 공간)\n",
    "z_cvae = np.random.randn(5000, 10) + 2  # CVAE는 특정 방향으로 분포가 이동할 가능성이 있음\n",
    "\n",
    "# 샘플링 (속도 개선)\n",
    "num_samples = 1000\n",
    "indices = np.random.choice(len(z_vae), num_samples, replace=False)\n",
    "z_vae_sampled = z_vae[indices]\n",
    "z_cvae_sampled = z_cvae[indices]\n",
    "\n",
    "# PCA 변환\n",
    "pca_vae = PCA(n_components=2).fit_transform(z_vae_sampled)\n",
    "pca_cvae = PCA(n_components=2).fit_transform(z_cvae_sampled)\n",
    "\n",
    "# t-SNE 변환\n",
    "tsne_vae = TSNE(n_components=2, perplexity=20, n_iter=500, random_state=42).fit_transform(z_vae_sampled)\n",
    "tsne_cvae = TSNE(n_components=2, perplexity=20, n_iter=500, random_state=42).fit_transform(z_cvae_sampled)\n",
    "\n",
    "# UMAP 변환\n",
    "umap_vae = umap.UMAP(n_components=2, random_state=42).fit_transform(z_vae_sampled)\n",
    "umap_cvae = umap.UMAP(n_components=2, random_state=42).fit_transform(z_cvae_sampled)\n",
    "\n",
    "# 시각화\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axs[0].scatter(pca_vae[:, 0], pca_vae[:, 1], alpha=0.5, label=\"VAE\")\n",
    "axs[0].scatter(pca_cvae[:, 0], pca_cvae[:, 1], alpha=0.5, label=\"CVAE\")\n",
    "axs[0].set_title(\"PCA\")\n",
    "\n",
    "axs[1].scatter(tsne_vae[:, 0], tsne_vae[:, 1], alpha=0.5, label=\"VAE\")\n",
    "axs[1].scatter(tsne_cvae[:, 0], tsne_cvae[:, 1], alpha=0.5, label=\"CVAE\")\n",
    "axs[1].set_title(\"t-SNE\")\n",
    "\n",
    "axs[2].scatter(umap_vae[:, 0], umap_vae[:, 1], alpha=0.5, label=\"VAE\")\n",
    "axs[2].scatter(umap_cvae[:, 0], umap_cvae[:, 1], alpha=0.5, label=\"CVAE\")\n",
    "axs[2].set_title(\"UMAP\")\n",
    "\n",
    "for ax in axs:\n",
    "    ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3b14c4-ba35-4e86-9abd-f01972abb514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 예제 데이터: VAE 및 CVAE의 정상 데이터 및 이상 데이터 Reconstruction Error\n",
    "recon_error_vae_normal = np.random.normal(loc=0.02, scale=0.005, size=500)\n",
    "recon_error_vae_anomaly = np.random.normal(loc=0.06, scale=0.01, size=100)\n",
    "\n",
    "recon_error_cvae_normal = np.random.normal(loc=0.01, scale=0.004, size=500)\n",
    "recon_error_cvae_anomaly = np.random.normal(loc=0.08, scale=0.012, size=100)\n",
    "\n",
    "# Box Plot으로 비교\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([\n",
    "    recon_error_vae_normal, recon_error_vae_anomaly,\n",
    "    recon_error_cvae_normal, recon_error_cvae_anomaly\n",
    "], labels=[\"VAE Normal\", \"VAE Anomaly\", \"CVAE Normal\", \"CVAE Anomaly\"])\n",
    "\n",
    "plt.title(\"Reconstruction Error Comparison (VAE vs CVAE)\")\n",
    "plt.ylabel(\"Reconstruction Error\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec2cfd-33d8-4c37-8ee4-b2a4b5366c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "# K-Means 클러스터링\n",
    "kmeans_vae = KMeans(n_clusters=3, random_state=42).fit(z_vae_sampled)\n",
    "kmeans_cvae = KMeans(n_clusters=3, random_state=42).fit(z_cvae_sampled)\n",
    "\n",
    "# DBSCAN 클러스터링\n",
    "dbscan_vae = DBSCAN(eps=0.5, min_samples=5).fit(z_vae_sampled)\n",
    "dbscan_cvae = DBSCAN(eps=0.5, min_samples=5).fit(z_cvae_sampled)\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(z_vae_sampled[:, 0], z_vae_sampled[:, 1], c=kmeans_vae.labels_, cmap=\"viridis\", alpha=0.5)\n",
    "plt.title(\"K-Means Clustering on VAE Latent Space\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(z_cvae_sampled[:, 0], z_cvae_sampled[:, 1], c=kmeans_cvae.labels_, cmap=\"viridis\", alpha=0.5)\n",
    "plt.title(\"K-Means Clustering on CVAE Latent Space\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
